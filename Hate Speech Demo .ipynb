{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4578c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09ffdf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 73, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhate-by-Kabada.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 73, saw 2\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('hate-by-Kabada.csv', encoding = 'utf-8')\n",
    "#,count,hate_speech,offensive_language,neither,class,tweet\n",
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc706fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0          2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1          1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2          1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3          1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4          1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "...      ...                                                ...\n",
       "24778      1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "24779      2  you've gone and broke the wrong heart baby, an...\n",
       "24780      1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "24781      1              youu got wild bitches tellin you lies\n",
       "24782      2  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "\n",
       "[24783 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_transformed = df[['class', 'tweet']]\n",
    "y = (dt_transformed.iloc[:, :-1].values).ravel()\n",
    "dt_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82fe104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22304, 2), (2479, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividindo o df em treino e teste\n",
    "df_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d38c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m(df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      2\u001b[0m corpus\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = preprocessing(df_train['tweet'].values)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treino e validação do corpus\n",
    "c_train, c_vad, y_train, y_vad = train_test_split(corpus, df_train['class'], test_size = 0.10, random_state = 42, stratify=df_train['class'])\n",
    "c_train.shape, c_vad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e28f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(c_train)\n",
    "sequences = tokenizer.texts_to_sequences(c_train)\n",
    "tokenizer.fit_on_texts(c_vad)\n",
    "sequences = tokenizer.texts_to_sequences(c_vad)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['tweet'].values.tolist()\n",
    "labels = df['class'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings\n",
    "embedding_dim = 100\n",
    "word_vectors = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(df['class']).values\n",
    "#y=pd.get_dummies(data['Intent']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66afb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create embedding matrix\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model.\n",
    "#model = gensim.models.Word2Vec.load('Aoword2vec.bin')\n",
    "#w2v_model = gensim.models.KeyedVectors.load_word2vec_format('Aoword2vec.bin', binary=True)\n",
    "\n",
    "# Define parameters\n",
    "max_sequence_length = 300\n",
    "num_classes = 3\n",
    "embedding_dim = 100\n",
    "num_filters = 32\n",
    "dropout_prob = 0.5\n",
    "hidden_dims = 64\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "labels = np_utils.to_categorical(np.asarray(labels))\n",
    "# Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f13861",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cf579",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c05d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48038bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b750edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ba889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "inputs = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "embedding = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True)(inputs)\n",
    "conv_0 = Conv1D(num_filters, 3, activation='relu')(embedding)\n",
    "maxpool_0 = MaxPooling1D(max_sequence_length - 3 + 1)(conv_0)\n",
    "dropout_0 = Dropout(dropout_prob)(maxpool_0)\n",
    "conv_1 = Conv1D(num_filters, 4, activation='relu')(embedding)\n",
    "maxpool_1 = MaxPooling1D(max_sequence_length - 4 + 1)(conv_1)\n",
    "dropout_1 = Dropout(dropout_prob)(maxpool_1)\n",
    "conv_2 = Conv1D(num_filters, 5, activation='relu')(embedding)\n",
    "maxpool_2 = MaxPooling1D(max_sequence_length - 5 + 1)(conv_2)\n",
    "dropout_2 = Dropout(dropout_prob)(maxpool_2)\n",
    "merged_tensor = concatenate([dropout_0, dropout_1, dropout_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "dense = Dense(hidden_dims, activation='relu')(flatten)\n",
    "dropout = Dropout(dropout_prob)(dense)\n",
    "output = Dense(num_classes, activation='sigmoid')(dropout)\n",
    "cnnmodel = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "354567c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 300, 100)     1434800     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 298, 32)      9632        ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 297, 32)      12832       ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 296, 32)      16032       ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 1, 32)       0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 1, 32)       0           ['conv1d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooling1D  (None, 1, 32)       0           ['conv1d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 1, 32)        0           ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 1, 32)        0           ['max_pooling1d_10[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 1, 32)        0           ['max_pooling1d_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 3, 32)        0           ['dropout_12[0][0]',             \n",
      "                                                                  'dropout_13[0][0]',             \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 96)           0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           6208        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 64)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3)            195         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,479,699\n",
      "Trainable params: 1,479,699\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile CNN model\n",
    "cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "263ee0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "175/175 [==============================] - 47s 271ms/step - loss: 0.3395 - accuracy: 0.8876\n",
      "Epoch 2/2\n",
      "175/175 [==============================] - 47s 270ms/step - loss: 0.2892 - accuracy: 0.9068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26f33bc51c0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train CNN model\n",
    "cnnmodel.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs) #validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e4beaa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [118]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features using CNN model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m get_dense_layer_output \u001b[38;5;241m=\u001b[39m Model(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m, model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      3\u001b[0m X_train_features \u001b[38;5;241m=\u001b[39m get_dense_layer_output\u001b[38;5;241m.\u001b[39mpredict(x_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_test_features \u001b[38;5;241m=\u001b[39m get_dense_layer_output\u001b[38;5;241m.\u001b[39mpredict(x_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'input'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features using CNN model\n",
    "get_dense_layer_output = Model(model.input, model.layers[-3].output)\n",
    "X_train_features = get_dense_layer_output.predict(x_train, verbose=1)\n",
    "X_test_features = get_dense_layer_output.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LR model on extracted features\n",
    "#clf = LogisticRegression(C=lr, penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(x_train, y_train.ravel())\n",
    "#y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = vectorizer.fit_transform(x_train)\n",
    "#X_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "#lr_model = LogisticRegression()\n",
    "#lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635088b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model = LogisticRegression()\n",
    "# Train LR model\n",
    "#lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b808c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b27d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0          2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1          1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2          1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3          1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4          1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "...      ...                                                ...\n",
       "24778      1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "24779      2  you've gone and broke the wrong heart baby, an...\n",
       "24780      1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "24781      1              youu got wild bitches tellin you lies\n",
       "24782      2  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "\n",
       "[24783 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_transformed = df[['class', 'tweet']]\n",
    "y = (dt_transformed.iloc[:, :-1].values).ravel()\n",
    "dt_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ff1cde75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22304, 2), (2479, 2))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividindo o df em treino e teste\n",
    "df_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7a62c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5b38cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    corpus = []\n",
    "    for tweet in data:\n",
    "        review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n",
    "        review = re.sub('RT', ' ', review)\n",
    "        review = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", review)\n",
    "        review = re.sub(r\"https?\", \" \", review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        ps = PorterStemmer()\n",
    "        review = [ps.stem(word) for word in review if not word in set(all_stopwords) if len(word) > 2]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2e57db3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22304,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = preprocessing(df_train['tweet'].values)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "201dfc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20073,), (2231,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treino e validação do corpus\n",
    "c_train, c_vad, y_train, y_vad = train_test_split(corpus, df_train['class'], test_size = 0.10, random_state = 42, stratify=df_train['class'])\n",
    "c_train.shape, c_vad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0b6a7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(c_train, c_vad):\n",
    "    tweet_tokenizer = TweetTokenizer() \n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, max_features = 1010)\n",
    "    X_train = vectorizer.fit_transform(c_train).toarray()\n",
    "    X_vad = vectorizer.transform(c_vad).toarray()\n",
    "    return X_train, X_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f8e80e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20073, 1010), (2231, 1010))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_vad = tokenize(c_train, c_vad)\n",
    "X_train.shape, X_vad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "44c7c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\n",
    "lr.fit(X_train, y_train.ravel())\n",
    "y_pred = model.predict(X_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b1450f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m ensemble_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2231\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     cnn_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray([test_data[i]]))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     lr_pred \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict_proba(np\u001b[38;5;241m.\u001b[39marray([test_data[i]]))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m     ensemble_pred \u001b[38;5;241m=\u001b[39m (cnn_pred \u001b[38;5;241m+\u001b[39m lr_pred) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensemble models\n",
    "ensemble_preds = []\n",
    "for i in range(2231):\n",
    "    cnn_pred = cnn_model.predict(np.array([test_data[i]]))[0][0]\n",
    "    lr_pred = lr_model.predict_proba(np.array([test_data[i]]))[0][1]\n",
    "    ensemble_pred = (cnn_pred + lr_pred) / 2\n",
    "    ensemble_preds.append(ensemble_pred)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "ensemble_preds = np.array(ensemble_preds)\n",
    "ensemble_preds[ensemble_preds >= 0.5] = 1\n",
    "ensemble_preds[ensemble_preds < 0.5] = 0\n",
    "ensemble_acc = accuracy_score(test_labels, ensemble_preds)\n",
    "print('Ensemble model accuracy:', ensemble_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_features)\n",
    "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print('Accuracy:', accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_shaped = np.ravel(y_train) \n",
    "x_train_shaped = np.ravel(X_train_features)\n",
    "clf.fit(X_train_features, y_train_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
