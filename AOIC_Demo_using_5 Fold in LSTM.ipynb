{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,LSTM,Embedding,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import re\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Intent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10561, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kiyyoo dirama oso  eguti jirun na darbe tari a...</td>\n",
       "      <td>Question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meeti ree kuta 28ffaa</td>\n",
       "      <td>Question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kutaa digdami sadetaffa isiin egee dadhabee.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kutaa digdami sadetaffaa fiidagaa maali nuuraa...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nuuf jabbadhakaa warii Akka kotti kiyyoo jaala...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Intent\n",
       "0  Kiyyoo dirama oso  eguti jirun na darbe tari a...  Question \n",
       "1                              Meeti ree kuta 28ffaa  Question \n",
       "2       Kutaa digdami sadetaffa isiin egee dadhabee.   Negative\n",
       "3  Kutaa digdami sadetaffaa fiidagaa maali nuuraa...   Negative\n",
       "4  Nuuf jabbadhakaa warii Akka kotti kiyyoo jaala...   Positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('intent12-corrected.csv',delimiter = ';',names=['Text','Intent'],encoding=\"utf-8\")\n",
    "data.sample(frac=1).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Intent', ylabel='count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Intent', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive       4301\n",
       "Negative       3271\n",
       "Suggestion     1686\n",
       "Question        718\n",
       "Wish            454\n",
       "Name: Intent, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Intent\"].value_counts()\n",
    "#print(len('Gosa_miira'))\n",
    "#print(len('jechoota'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = sen.lower()\n",
    "    sentence = re.sub(r'wo*w','wow', sentence)\n",
    "    sentence = re.sub(r'uu*f','uff', sentence)\n",
    "    return sentence\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(Text):\n",
    "    return TAG_RE.sub('', Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(review_words):\n",
    "    with open('stopwords.txt')as stopfile:\n",
    "        stopwords=stopfile.read()\n",
    "        list=stopwords.split()\n",
    "        print(list)\n",
    "        with open(\"intent1.csv\") as workfile:\n",
    "            read_data=workfile.read()\n",
    "            data=read_data.split()\n",
    "            print(data)\n",
    "            for word1 in list:\n",
    "                for word2 in data:\n",
    "                    if word1==word2:\n",
    "                        return data.remove(list)\n",
    "                        print(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        kiyyoo dirama oso  eguti jirun na darbe tari a...\n",
       "1                                    meeti ree kuta 28ffaa\n",
       "2             kutaa digdami sadetaffa isiin egee dadhabee.\n",
       "3        kutaa digdami sadetaffaa fiidagaa maali nuuraa...\n",
       "4        nuuf jabbadhakaa warii akka kotti kiyyoo jaala...\n",
       "                               ...                        \n",
       "10556    isheenis namoota dogoggoraa waliin lafa dogogg...\n",
       "10557    vidiyichis battalumatti qoodamuun marsariitiiw...\n",
       "10558    gochiwwan qaamaa warraabbicha irratti ture bah...\n",
       "10559              jechi gurbbichaa altokkicha dhagahame. \n",
       "10560                           viidiyoo waraabaa jirtaa? \n",
       "Name: Text, Length: 10561, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,  198, 3008, 3009,\n",
       "          37,  194,  146,  325],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  432, 3297],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         159, 3298,  319,  413],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  159,\n",
       "        3298,  154, 3094,   46],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,   75, 3299,\n",
       "           4, 3095,  198, 3300]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words=4000,split=\" \")\n",
    "tokenizer.fit_on_texts(data['Text'].values)\n",
    "x=tokenizer.texts_to_sequences(data['Text'].values)\n",
    "x=pad_sequences(x)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question  [0 0 1 0 0]\n",
      "Question  [0 0 1 0 0]\n",
      "Negative [1 0 0 0 0]\n",
      "Negative [1 0 0 0 0]\n",
      "Positive [0 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.get_dummies(data['Intent']).values\n",
    "[print(data['Intent'][i],y[i]) for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/32\n",
      "132/132 - 35s - loss: 1.3216 - accuracy: 0.4205 - val_loss: 1.1738 - val_accuracy: 0.5121 - 35s/epoch - 261ms/step\n",
      "Epoch 2/32\n",
      "132/132 - 13s - loss: 0.8848 - accuracy: 0.6521 - val_loss: 0.7389 - val_accuracy: 0.7085 - 13s/epoch - 101ms/step\n",
      "Epoch 3/32\n",
      "132/132 - 13s - loss: 0.6067 - accuracy: 0.7644 - val_loss: 0.5968 - val_accuracy: 0.7605 - 13s/epoch - 100ms/step\n",
      "Epoch 4/32\n",
      "132/132 - 13s - loss: 0.4721 - accuracy: 0.8156 - val_loss: 0.5413 - val_accuracy: 0.7918 - 13s/epoch - 98ms/step\n",
      "Epoch 5/32\n",
      "132/132 - 13s - loss: 0.4085 - accuracy: 0.8389 - val_loss: 0.5384 - val_accuracy: 0.7984 - 13s/epoch - 100ms/step\n",
      "Epoch 6/32\n",
      "132/132 - 13s - loss: 0.3743 - accuracy: 0.8509 - val_loss: 0.4590 - val_accuracy: 0.8112 - 13s/epoch - 99ms/step\n",
      "Epoch 7/32\n",
      "132/132 - 13s - loss: 0.3427 - accuracy: 0.8620 - val_loss: 0.4314 - val_accuracy: 0.8315 - 13s/epoch - 99ms/step\n",
      "Epoch 8/32\n",
      "132/132 - 13s - loss: 0.3195 - accuracy: 0.8686 - val_loss: 0.4141 - val_accuracy: 0.8306 - 13s/epoch - 101ms/step\n",
      "Epoch 9/32\n",
      "132/132 - 16s - loss: 0.2788 - accuracy: 0.8870 - val_loss: 0.4109 - val_accuracy: 0.8443 - 16s/epoch - 119ms/step\n",
      "Epoch 10/32\n",
      "132/132 - 15s - loss: 0.2793 - accuracy: 0.8826 - val_loss: 0.4269 - val_accuracy: 0.8410 - 15s/epoch - 111ms/step\n",
      "Epoch 11/32\n",
      "132/132 - 14s - loss: 0.2613 - accuracy: 0.8906 - val_loss: 0.4048 - val_accuracy: 0.8353 - 14s/epoch - 105ms/step\n",
      "Epoch 12/32\n",
      "132/132 - 13s - loss: 0.2599 - accuracy: 0.8885 - val_loss: 0.4045 - val_accuracy: 0.8452 - 13s/epoch - 100ms/step\n",
      "Epoch 13/32\n",
      "132/132 - 13s - loss: 0.2519 - accuracy: 0.8902 - val_loss: 0.4071 - val_accuracy: 0.8429 - 13s/epoch - 102ms/step\n",
      "Epoch 14/32\n",
      "132/132 - 13s - loss: 0.2405 - accuracy: 0.8946 - val_loss: 0.4042 - val_accuracy: 0.8580 - 13s/epoch - 101ms/step\n",
      "Epoch 15/32\n",
      "132/132 - 13s - loss: 0.2388 - accuracy: 0.8949 - val_loss: 0.4251 - val_accuracy: 0.8405 - 13s/epoch - 98ms/step\n",
      "Epoch 16/32\n",
      "132/132 - 12s - loss: 0.2244 - accuracy: 0.9018 - val_loss: 0.3900 - val_accuracy: 0.8481 - 12s/epoch - 94ms/step\n",
      "Epoch 17/32\n",
      "132/132 - 13s - loss: 0.2270 - accuracy: 0.8991 - val_loss: 0.3989 - val_accuracy: 0.8457 - 13s/epoch - 96ms/step\n",
      "Epoch 18/32\n",
      "132/132 - 13s - loss: 0.2156 - accuracy: 0.9040 - val_loss: 0.4165 - val_accuracy: 0.8509 - 13s/epoch - 100ms/step\n",
      "Epoch 19/32\n",
      "132/132 - 14s - loss: 0.2126 - accuracy: 0.9026 - val_loss: 0.4013 - val_accuracy: 0.8490 - 14s/epoch - 107ms/step\n",
      "Epoch 20/32\n",
      "132/132 - 14s - loss: 0.2086 - accuracy: 0.9066 - val_loss: 0.4132 - val_accuracy: 0.8528 - 14s/epoch - 106ms/step\n",
      "Epoch 21/32\n",
      "132/132 - 14s - loss: 0.2111 - accuracy: 0.9014 - val_loss: 0.4308 - val_accuracy: 0.8528 - 14s/epoch - 103ms/step\n",
      "Epoch 22/32\n",
      "132/132 - 13s - loss: 0.2234 - accuracy: 0.9022 - val_loss: 0.4198 - val_accuracy: 0.8490 - 13s/epoch - 102ms/step\n",
      "Epoch 23/32\n",
      "132/132 - 13s - loss: 0.2044 - accuracy: 0.9084 - val_loss: 0.4320 - val_accuracy: 0.8476 - 13s/epoch - 101ms/step\n",
      "Epoch 24/32\n",
      "132/132 - 13s - loss: 0.1978 - accuracy: 0.9115 - val_loss: 0.4368 - val_accuracy: 0.8528 - 13s/epoch - 101ms/step\n",
      "Epoch 25/32\n",
      "132/132 - 14s - loss: 0.1934 - accuracy: 0.9144 - val_loss: 0.4324 - val_accuracy: 0.8495 - 14s/epoch - 106ms/step\n",
      "Epoch 26/32\n",
      "132/132 - 14s - loss: 0.1962 - accuracy: 0.9136 - val_loss: 0.4515 - val_accuracy: 0.8462 - 14s/epoch - 103ms/step\n",
      "Epoch 27/32\n",
      "132/132 - 13s - loss: 0.1957 - accuracy: 0.9107 - val_loss: 0.4422 - val_accuracy: 0.8452 - 13s/epoch - 99ms/step\n",
      "Epoch 28/32\n",
      "132/132 - 13s - loss: 0.2063 - accuracy: 0.9058 - val_loss: 0.4279 - val_accuracy: 0.8504 - 13s/epoch - 97ms/step\n",
      "Epoch 29/32\n",
      "132/132 - 13s - loss: 0.1851 - accuracy: 0.9162 - val_loss: 0.4630 - val_accuracy: 0.8509 - 13s/epoch - 100ms/step\n",
      "Epoch 30/32\n",
      "132/132 - 13s - loss: 0.1893 - accuracy: 0.9135 - val_loss: 0.4590 - val_accuracy: 0.8457 - 13s/epoch - 99ms/step\n",
      "Epoch 31/32\n",
      "132/132 - 14s - loss: 0.1857 - accuracy: 0.9132 - val_loss: 0.4353 - val_accuracy: 0.8467 - 14s/epoch - 109ms/step\n",
      "Epoch 32/32\n",
      "132/132 - 15s - loss: 0.1786 - accuracy: 0.9160 - val_loss: 0.4584 - val_accuracy: 0.8519 - 15s/epoch - 110ms/step\n",
      "Score for fold 1: loss of 0.45839089155197144; accuracy of 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.45839089155197144 - Accuracy: 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 85.18694043159485 (+- 0.0)\n",
      "> Loss: 0.45839089155197144\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/32\n",
      "133/133 - 38s - loss: 1.3396 - accuracy: 0.4137 - val_loss: 1.2120 - val_accuracy: 0.4972 - 38s/epoch - 285ms/step\n",
      "Epoch 2/32\n",
      "133/133 - 15s - loss: 0.9211 - accuracy: 0.6339 - val_loss: 0.7521 - val_accuracy: 0.7211 - 15s/epoch - 112ms/step\n",
      "Epoch 3/32\n",
      "133/133 - 15s - loss: 0.6223 - accuracy: 0.7583 - val_loss: 0.6074 - val_accuracy: 0.7576 - 15s/epoch - 109ms/step\n",
      "Epoch 4/32\n",
      "133/133 - 15s - loss: 0.4897 - accuracy: 0.8076 - val_loss: 0.5549 - val_accuracy: 0.7902 - 15s/epoch - 110ms/step\n",
      "Epoch 5/32\n",
      "133/133 - 14s - loss: 0.4155 - accuracy: 0.8377 - val_loss: 0.4951 - val_accuracy: 0.8092 - 14s/epoch - 107ms/step\n",
      "Epoch 6/32\n",
      "133/133 - 13s - loss: 0.3671 - accuracy: 0.8523 - val_loss: 0.4809 - val_accuracy: 0.8182 - 13s/epoch - 101ms/step\n",
      "Epoch 7/32\n",
      "133/133 - 14s - loss: 0.3417 - accuracy: 0.8610 - val_loss: 0.4601 - val_accuracy: 0.8187 - 14s/epoch - 105ms/step\n",
      "Epoch 8/32\n",
      "133/133 - 13s - loss: 0.3145 - accuracy: 0.8690 - val_loss: 0.4542 - val_accuracy: 0.8243 - 13s/epoch - 101ms/step\n",
      "Epoch 9/32\n",
      "133/133 - 14s - loss: 0.2992 - accuracy: 0.8793 - val_loss: 0.4327 - val_accuracy: 0.8362 - 14s/epoch - 103ms/step\n",
      "Epoch 10/32\n",
      "133/133 - 14s - loss: 0.2959 - accuracy: 0.8764 - val_loss: 0.4356 - val_accuracy: 0.8371 - 14s/epoch - 103ms/step\n",
      "Epoch 11/32\n",
      "133/133 - 15s - loss: 0.2780 - accuracy: 0.8845 - val_loss: 0.4308 - val_accuracy: 0.8333 - 15s/epoch - 111ms/step\n",
      "Epoch 12/32\n",
      "133/133 - 15s - loss: 0.2645 - accuracy: 0.8915 - val_loss: 0.4318 - val_accuracy: 0.8348 - 15s/epoch - 115ms/step\n",
      "Epoch 13/32\n",
      "133/133 - 15s - loss: 0.2483 - accuracy: 0.8951 - val_loss: 0.4352 - val_accuracy: 0.8395 - 15s/epoch - 110ms/step\n",
      "Epoch 14/32\n",
      "133/133 - 14s - loss: 0.2473 - accuracy: 0.8921 - val_loss: 0.4341 - val_accuracy: 0.8475 - 14s/epoch - 108ms/step\n",
      "Epoch 15/32\n",
      "133/133 - 14s - loss: 0.2357 - accuracy: 0.8987 - val_loss: 0.4648 - val_accuracy: 0.8400 - 14s/epoch - 102ms/step\n",
      "Epoch 16/32\n",
      "133/133 - 14s - loss: 0.2313 - accuracy: 0.8979 - val_loss: 0.4510 - val_accuracy: 0.8371 - 14s/epoch - 104ms/step\n",
      "Epoch 17/32\n",
      "133/133 - 13s - loss: 0.2261 - accuracy: 0.9046 - val_loss: 0.4319 - val_accuracy: 0.8442 - 13s/epoch - 99ms/step\n",
      "Epoch 18/32\n",
      "133/133 - 13s - loss: 0.2147 - accuracy: 0.9034 - val_loss: 0.4345 - val_accuracy: 0.8447 - 13s/epoch - 99ms/step\n",
      "Epoch 19/32\n",
      "133/133 - 13s - loss: 0.2196 - accuracy: 0.9035 - val_loss: 0.4556 - val_accuracy: 0.8400 - 13s/epoch - 98ms/step\n",
      "Epoch 20/32\n",
      "133/133 - 15s - loss: 0.2152 - accuracy: 0.9046 - val_loss: 0.4485 - val_accuracy: 0.8419 - 15s/epoch - 110ms/step\n",
      "Epoch 21/32\n",
      "133/133 - 16s - loss: 0.2092 - accuracy: 0.9074 - val_loss: 0.4340 - val_accuracy: 0.8461 - 16s/epoch - 117ms/step\n",
      "Epoch 22/32\n",
      "133/133 - 16s - loss: 0.2042 - accuracy: 0.9071 - val_loss: 0.4518 - val_accuracy: 0.8423 - 16s/epoch - 119ms/step\n",
      "Epoch 23/32\n",
      "133/133 - 14s - loss: 0.2009 - accuracy: 0.9095 - val_loss: 0.4671 - val_accuracy: 0.8433 - 14s/epoch - 105ms/step\n",
      "Epoch 24/32\n",
      "133/133 - 15s - loss: 0.2028 - accuracy: 0.9074 - val_loss: 0.4521 - val_accuracy: 0.8513 - 15s/epoch - 114ms/step\n",
      "Epoch 25/32\n",
      "133/133 - 14s - loss: 0.1995 - accuracy: 0.9090 - val_loss: 0.4380 - val_accuracy: 0.8527 - 14s/epoch - 108ms/step\n",
      "Epoch 26/32\n",
      "133/133 - 14s - loss: 0.1921 - accuracy: 0.9141 - val_loss: 0.4422 - val_accuracy: 0.8499 - 14s/epoch - 107ms/step\n",
      "Epoch 27/32\n",
      "133/133 - 15s - loss: 0.2074 - accuracy: 0.9065 - val_loss: 0.4432 - val_accuracy: 0.8452 - 15s/epoch - 111ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/32\n",
      "133/133 - 15s - loss: 0.1921 - accuracy: 0.9085 - val_loss: 0.4653 - val_accuracy: 0.8527 - 15s/epoch - 113ms/step\n",
      "Epoch 29/32\n",
      "133/133 - 15s - loss: 0.1915 - accuracy: 0.9117 - val_loss: 0.4513 - val_accuracy: 0.8509 - 15s/epoch - 111ms/step\n",
      "Epoch 30/32\n",
      "133/133 - 16s - loss: 0.1876 - accuracy: 0.9140 - val_loss: 0.4606 - val_accuracy: 0.8513 - 16s/epoch - 119ms/step\n",
      "Epoch 31/32\n",
      "133/133 - 15s - loss: 0.1849 - accuracy: 0.9166 - val_loss: 0.4730 - val_accuracy: 0.8546 - 15s/epoch - 116ms/step\n",
      "Epoch 32/32\n",
      "133/133 - 15s - loss: 0.1834 - accuracy: 0.9187 - val_loss: 0.4539 - val_accuracy: 0.8546 - 15s/epoch - 115ms/step\n",
      "Score for fold 2: loss of 0.4538726806640625; accuracy of 85.46401262283325%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.45839089155197144 - Accuracy: 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4538726806640625 - Accuracy: 85.46401262283325%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 85.32547652721405 (+- 0.13853609561920166)\n",
      "> Loss: 0.45613178610801697\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/32\n",
      "133/133 - 38s - loss: 1.3222 - accuracy: 0.4204 - val_loss: 1.1519 - val_accuracy: 0.5341 - 38s/epoch - 286ms/step\n",
      "Epoch 2/32\n",
      "133/133 - 15s - loss: 0.8854 - accuracy: 0.6603 - val_loss: 0.7496 - val_accuracy: 0.7003 - 15s/epoch - 113ms/step\n",
      "Epoch 3/32\n",
      "133/133 - 16s - loss: 0.6316 - accuracy: 0.7575 - val_loss: 0.5898 - val_accuracy: 0.7642 - 16s/epoch - 123ms/step\n",
      "Epoch 4/32\n",
      "133/133 - 15s - loss: 0.4788 - accuracy: 0.8216 - val_loss: 0.5111 - val_accuracy: 0.8040 - 15s/epoch - 109ms/step\n",
      "Epoch 5/32\n",
      "133/133 - 15s - loss: 0.3965 - accuracy: 0.8447 - val_loss: 0.4973 - val_accuracy: 0.8097 - 15s/epoch - 112ms/step\n",
      "Epoch 6/32\n",
      "133/133 - 15s - loss: 0.3610 - accuracy: 0.8596 - val_loss: 0.4528 - val_accuracy: 0.8149 - 15s/epoch - 111ms/step\n",
      "Epoch 7/32\n",
      "133/133 - 14s - loss: 0.3187 - accuracy: 0.8712 - val_loss: 0.4565 - val_accuracy: 0.8300 - 14s/epoch - 108ms/step\n",
      "Epoch 8/32\n",
      "133/133 - 14s - loss: 0.3040 - accuracy: 0.8762 - val_loss: 0.4509 - val_accuracy: 0.8267 - 14s/epoch - 106ms/step\n",
      "Epoch 9/32\n",
      "133/133 - 14s - loss: 0.2825 - accuracy: 0.8819 - val_loss: 0.4229 - val_accuracy: 0.8314 - 14s/epoch - 109ms/step\n",
      "Epoch 10/32\n",
      "133/133 - 15s - loss: 0.2612 - accuracy: 0.8903 - val_loss: 0.4614 - val_accuracy: 0.8362 - 15s/epoch - 116ms/step\n",
      "Epoch 11/32\n",
      "133/133 - 15s - loss: 0.2636 - accuracy: 0.8921 - val_loss: 0.4282 - val_accuracy: 0.8291 - 15s/epoch - 110ms/step\n",
      "Epoch 12/32\n",
      "133/133 - 16s - loss: 0.2445 - accuracy: 0.8928 - val_loss: 0.4389 - val_accuracy: 0.8438 - 16s/epoch - 121ms/step\n",
      "Epoch 13/32\n",
      "133/133 - 15s - loss: 0.2549 - accuracy: 0.8936 - val_loss: 0.4480 - val_accuracy: 0.8348 - 15s/epoch - 112ms/step\n",
      "Epoch 14/32\n",
      "133/133 - 14s - loss: 0.2325 - accuracy: 0.9041 - val_loss: 0.4290 - val_accuracy: 0.8404 - 14s/epoch - 108ms/step\n",
      "Epoch 15/32\n",
      "133/133 - 14s - loss: 0.2315 - accuracy: 0.9050 - val_loss: 0.4535 - val_accuracy: 0.8352 - 14s/epoch - 106ms/step\n",
      "Epoch 16/32\n",
      "133/133 - 14s - loss: 0.2233 - accuracy: 0.9058 - val_loss: 0.4309 - val_accuracy: 0.8423 - 14s/epoch - 106ms/step\n",
      "Epoch 17/32\n",
      "133/133 - 14s - loss: 0.2163 - accuracy: 0.9059 - val_loss: 0.4456 - val_accuracy: 0.8338 - 14s/epoch - 107ms/step\n",
      "Epoch 18/32\n",
      "133/133 - 14s - loss: 0.2119 - accuracy: 0.9059 - val_loss: 0.4446 - val_accuracy: 0.8428 - 14s/epoch - 106ms/step\n",
      "Epoch 19/32\n",
      "133/133 - 14s - loss: 0.2077 - accuracy: 0.9087 - val_loss: 0.4720 - val_accuracy: 0.8404 - 14s/epoch - 106ms/step\n",
      "Epoch 20/32\n",
      "133/133 - 15s - loss: 0.2062 - accuracy: 0.9074 - val_loss: 0.4603 - val_accuracy: 0.8390 - 15s/epoch - 112ms/step\n",
      "Epoch 21/32\n",
      "133/133 - 14s - loss: 0.2114 - accuracy: 0.9069 - val_loss: 0.4438 - val_accuracy: 0.8362 - 14s/epoch - 106ms/step\n",
      "Epoch 22/32\n",
      "133/133 - 15s - loss: 0.2117 - accuracy: 0.9080 - val_loss: 0.4399 - val_accuracy: 0.8471 - 15s/epoch - 111ms/step\n",
      "Epoch 23/32\n",
      "133/133 - 14s - loss: 0.1901 - accuracy: 0.9141 - val_loss: 0.4765 - val_accuracy: 0.8471 - 14s/epoch - 106ms/step\n",
      "Epoch 24/32\n",
      "133/133 - 14s - loss: 0.1980 - accuracy: 0.9104 - val_loss: 0.4589 - val_accuracy: 0.8404 - 14s/epoch - 107ms/step\n",
      "Epoch 25/32\n",
      "133/133 - 14s - loss: 0.1887 - accuracy: 0.9153 - val_loss: 0.4939 - val_accuracy: 0.8438 - 14s/epoch - 106ms/step\n",
      "Epoch 26/32\n",
      "133/133 - 14s - loss: 0.2123 - accuracy: 0.9078 - val_loss: 0.4506 - val_accuracy: 0.8452 - 14s/epoch - 109ms/step\n",
      "Epoch 27/32\n",
      "133/133 - 14s - loss: 0.1887 - accuracy: 0.9151 - val_loss: 0.4624 - val_accuracy: 0.8357 - 14s/epoch - 104ms/step\n",
      "Epoch 28/32\n",
      "133/133 - 12s - loss: 0.1783 - accuracy: 0.9171 - val_loss: 0.4611 - val_accuracy: 0.8499 - 12s/epoch - 91ms/step\n",
      "Epoch 29/32\n",
      "133/133 - 12s - loss: 0.1804 - accuracy: 0.9153 - val_loss: 0.4903 - val_accuracy: 0.8433 - 12s/epoch - 90ms/step\n",
      "Epoch 30/32\n",
      "133/133 - 12s - loss: 0.1769 - accuracy: 0.9186 - val_loss: 0.4646 - val_accuracy: 0.8461 - 12s/epoch - 90ms/step\n",
      "Epoch 31/32\n",
      "133/133 - 13s - loss: 0.1773 - accuracy: 0.9185 - val_loss: 0.4830 - val_accuracy: 0.8504 - 13s/epoch - 96ms/step\n",
      "Epoch 32/32\n",
      "133/133 - 12s - loss: 0.1764 - accuracy: 0.9196 - val_loss: 0.4716 - val_accuracy: 0.8456 - 12s/epoch - 92ms/step\n",
      "Score for fold 3: loss of 0.47158581018447876; accuracy of 84.5643937587738%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.45839089155197144 - Accuracy: 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4538726806640625 - Accuracy: 85.46401262283325%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47158581018447876 - Accuracy: 84.5643937587738%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 85.0717822710673 (+- 0.3761866350585121)\n",
      "> Loss: 0.4612831274668376\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/32\n",
      "133/133 - 40s - loss: 1.3237 - accuracy: 0.4202 - val_loss: 1.1449 - val_accuracy: 0.5791 - 40s/epoch - 301ms/step\n",
      "Epoch 2/32\n",
      "133/133 - 15s - loss: 0.8871 - accuracy: 0.6563 - val_loss: 0.7798 - val_accuracy: 0.6998 - 15s/epoch - 116ms/step\n",
      "Epoch 3/32\n",
      "133/133 - 17s - loss: 0.6025 - accuracy: 0.7648 - val_loss: 0.5688 - val_accuracy: 0.7817 - 17s/epoch - 125ms/step\n",
      "Epoch 4/32\n",
      "133/133 - 16s - loss: 0.4959 - accuracy: 0.8061 - val_loss: 0.5075 - val_accuracy: 0.8068 - 16s/epoch - 122ms/step\n",
      "Epoch 5/32\n",
      "133/133 - 17s - loss: 0.4248 - accuracy: 0.8310 - val_loss: 0.4637 - val_accuracy: 0.8177 - 17s/epoch - 124ms/step\n",
      "Epoch 6/32\n",
      "133/133 - 16s - loss: 0.3918 - accuracy: 0.8432 - val_loss: 0.4547 - val_accuracy: 0.8201 - 16s/epoch - 122ms/step\n",
      "Epoch 7/32\n",
      "133/133 - 17s - loss: 0.3534 - accuracy: 0.8586 - val_loss: 0.4668 - val_accuracy: 0.8182 - 17s/epoch - 127ms/step\n",
      "Epoch 8/32\n",
      "133/133 - 17s - loss: 0.3518 - accuracy: 0.8581 - val_loss: 0.4448 - val_accuracy: 0.8253 - 17s/epoch - 126ms/step\n",
      "Epoch 9/32\n",
      "133/133 - 16s - loss: 0.3138 - accuracy: 0.8722 - val_loss: 0.4242 - val_accuracy: 0.8381 - 16s/epoch - 118ms/step\n",
      "Epoch 10/32\n",
      "133/133 - 16s - loss: 0.2925 - accuracy: 0.8801 - val_loss: 0.4195 - val_accuracy: 0.8357 - 16s/epoch - 119ms/step\n",
      "Epoch 11/32\n",
      "133/133 - 17s - loss: 0.3060 - accuracy: 0.8739 - val_loss: 0.4875 - val_accuracy: 0.8210 - 17s/epoch - 125ms/step\n",
      "Epoch 12/32\n",
      "133/133 - 16s - loss: 0.2971 - accuracy: 0.8789 - val_loss: 0.4012 - val_accuracy: 0.8376 - 16s/epoch - 118ms/step\n",
      "Epoch 13/32\n",
      "133/133 - 15s - loss: 0.2573 - accuracy: 0.8889 - val_loss: 0.4036 - val_accuracy: 0.8456 - 15s/epoch - 116ms/step\n",
      "Epoch 14/32\n",
      "133/133 - 15s - loss: 0.2489 - accuracy: 0.8916 - val_loss: 0.4069 - val_accuracy: 0.8438 - 15s/epoch - 114ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/32\n",
      "133/133 - 15s - loss: 0.2410 - accuracy: 0.8958 - val_loss: 0.4129 - val_accuracy: 0.8499 - 15s/epoch - 112ms/step\n",
      "Epoch 16/32\n",
      "133/133 - 15s - loss: 0.2356 - accuracy: 0.8957 - val_loss: 0.4053 - val_accuracy: 0.8513 - 15s/epoch - 116ms/step\n",
      "Epoch 17/32\n",
      "133/133 - 16s - loss: 0.2331 - accuracy: 0.8983 - val_loss: 0.4120 - val_accuracy: 0.8452 - 16s/epoch - 122ms/step\n",
      "Epoch 18/32\n",
      "133/133 - 15s - loss: 0.2277 - accuracy: 0.8989 - val_loss: 0.4144 - val_accuracy: 0.8456 - 15s/epoch - 115ms/step\n",
      "Epoch 19/32\n",
      "133/133 - 15s - loss: 0.2242 - accuracy: 0.9034 - val_loss: 0.4101 - val_accuracy: 0.8509 - 15s/epoch - 116ms/step\n",
      "Epoch 20/32\n",
      "133/133 - 16s - loss: 0.2107 - accuracy: 0.9063 - val_loss: 0.4348 - val_accuracy: 0.8485 - 16s/epoch - 121ms/step\n",
      "Epoch 21/32\n",
      "133/133 - 21s - loss: 0.2147 - accuracy: 0.9000 - val_loss: 0.4149 - val_accuracy: 0.8490 - 21s/epoch - 161ms/step\n",
      "Epoch 22/32\n",
      "133/133 - 22s - loss: 0.2064 - accuracy: 0.9046 - val_loss: 0.4211 - val_accuracy: 0.8532 - 22s/epoch - 162ms/step\n",
      "Epoch 23/32\n",
      "133/133 - 23s - loss: 0.2149 - accuracy: 0.9034 - val_loss: 0.4209 - val_accuracy: 0.8452 - 23s/epoch - 169ms/step\n",
      "Epoch 24/32\n",
      "133/133 - 21s - loss: 0.2046 - accuracy: 0.9057 - val_loss: 0.4095 - val_accuracy: 0.8494 - 21s/epoch - 161ms/step\n",
      "Epoch 25/32\n",
      "133/133 - 22s - loss: 0.1963 - accuracy: 0.9108 - val_loss: 0.4573 - val_accuracy: 0.8480 - 22s/epoch - 164ms/step\n",
      "Epoch 26/32\n",
      "133/133 - 22s - loss: 0.2009 - accuracy: 0.9085 - val_loss: 0.4335 - val_accuracy: 0.8513 - 22s/epoch - 163ms/step\n",
      "Epoch 27/32\n",
      "133/133 - 22s - loss: 0.1997 - accuracy: 0.9092 - val_loss: 0.4362 - val_accuracy: 0.8509 - 22s/epoch - 165ms/step\n",
      "Epoch 28/32\n",
      "133/133 - 21s - loss: 0.1935 - accuracy: 0.9096 - val_loss: 0.4323 - val_accuracy: 0.8518 - 21s/epoch - 161ms/step\n",
      "Epoch 29/32\n",
      "133/133 - 21s - loss: 0.1908 - accuracy: 0.9099 - val_loss: 0.4600 - val_accuracy: 0.8471 - 21s/epoch - 160ms/step\n",
      "Epoch 30/32\n",
      "133/133 - 21s - loss: 0.1936 - accuracy: 0.9091 - val_loss: 0.4384 - val_accuracy: 0.8537 - 21s/epoch - 160ms/step\n",
      "Epoch 31/32\n",
      "133/133 - 21s - loss: 0.1896 - accuracy: 0.9117 - val_loss: 0.4503 - val_accuracy: 0.8494 - 21s/epoch - 161ms/step\n",
      "Epoch 32/32\n",
      "133/133 - 21s - loss: 0.1835 - accuracy: 0.9131 - val_loss: 0.4582 - val_accuracy: 0.8622 - 21s/epoch - 158ms/step\n",
      "Score for fold 4: loss of 0.45820730924606323; accuracy of 86.22159361839294%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.45839089155197144 - Accuracy: 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4538726806640625 - Accuracy: 85.46401262283325%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47158581018447876 - Accuracy: 84.5643937587738%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.45820730924606323 - Accuracy: 86.22159361839294%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 85.35923510789871 (+- 0.5949997382392378)\n",
      "> Loss: 0.460514172911644\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/32\n",
      "133/133 - 55s - loss: 1.3257 - accuracy: 0.4124 - val_loss: 1.1917 - val_accuracy: 0.4891 - 55s/epoch - 410ms/step\n",
      "Epoch 2/32\n",
      "133/133 - 24s - loss: 0.9338 - accuracy: 0.6357 - val_loss: 0.7821 - val_accuracy: 0.7031 - 24s/epoch - 184ms/step\n",
      "Epoch 3/32\n",
      "133/133 - 24s - loss: 0.6363 - accuracy: 0.7564 - val_loss: 0.6282 - val_accuracy: 0.7580 - 24s/epoch - 183ms/step\n",
      "Epoch 4/32\n",
      "133/133 - 24s - loss: 0.4865 - accuracy: 0.8137 - val_loss: 0.5190 - val_accuracy: 0.7969 - 24s/epoch - 177ms/step\n",
      "Epoch 5/32\n",
      "133/133 - 24s - loss: 0.4076 - accuracy: 0.8414 - val_loss: 0.4733 - val_accuracy: 0.8243 - 24s/epoch - 177ms/step\n",
      "Epoch 6/32\n",
      "133/133 - 24s - loss: 0.3618 - accuracy: 0.8555 - val_loss: 0.4635 - val_accuracy: 0.8205 - 24s/epoch - 178ms/step\n",
      "Epoch 7/32\n",
      "133/133 - 24s - loss: 0.3276 - accuracy: 0.8692 - val_loss: 0.4465 - val_accuracy: 0.8224 - 24s/epoch - 178ms/step\n",
      "Epoch 8/32\n",
      "133/133 - 24s - loss: 0.3280 - accuracy: 0.8664 - val_loss: 0.4229 - val_accuracy: 0.8357 - 24s/epoch - 179ms/step\n",
      "Epoch 9/32\n",
      "133/133 - 24s - loss: 0.2917 - accuracy: 0.8808 - val_loss: 0.4290 - val_accuracy: 0.8329 - 24s/epoch - 181ms/step\n",
      "Epoch 10/32\n",
      "133/133 - 24s - loss: 0.2699 - accuracy: 0.8892 - val_loss: 0.4200 - val_accuracy: 0.8404 - 24s/epoch - 179ms/step\n",
      "Epoch 11/32\n",
      "133/133 - 24s - loss: 0.2546 - accuracy: 0.8935 - val_loss: 0.4200 - val_accuracy: 0.8400 - 24s/epoch - 178ms/step\n",
      "Epoch 12/32\n",
      "133/133 - 23s - loss: 0.2532 - accuracy: 0.8905 - val_loss: 0.4197 - val_accuracy: 0.8366 - 23s/epoch - 175ms/step\n",
      "Epoch 13/32\n",
      "133/133 - 24s - loss: 0.2360 - accuracy: 0.8990 - val_loss: 0.4662 - val_accuracy: 0.8324 - 24s/epoch - 177ms/step\n",
      "Epoch 14/32\n",
      "133/133 - 23s - loss: 0.2403 - accuracy: 0.8973 - val_loss: 0.4119 - val_accuracy: 0.8438 - 23s/epoch - 173ms/step\n",
      "Epoch 15/32\n",
      "133/133 - 23s - loss: 0.2426 - accuracy: 0.8927 - val_loss: 0.4339 - val_accuracy: 0.8409 - 23s/epoch - 173ms/step\n",
      "Epoch 16/32\n",
      "133/133 - 23s - loss: 0.2250 - accuracy: 0.9039 - val_loss: 0.4172 - val_accuracy: 0.8433 - 23s/epoch - 173ms/step\n",
      "Epoch 17/32\n",
      "133/133 - 23s - loss: 0.2185 - accuracy: 0.9013 - val_loss: 0.4120 - val_accuracy: 0.8471 - 23s/epoch - 171ms/step\n",
      "Epoch 18/32\n",
      "133/133 - 22s - loss: 0.2082 - accuracy: 0.9039 - val_loss: 0.4245 - val_accuracy: 0.8456 - 22s/epoch - 166ms/step\n",
      "Epoch 19/32\n",
      "133/133 - 22s - loss: 0.2066 - accuracy: 0.9114 - val_loss: 0.4175 - val_accuracy: 0.8456 - 22s/epoch - 167ms/step\n",
      "Epoch 20/32\n",
      "133/133 - 22s - loss: 0.2143 - accuracy: 0.9060 - val_loss: 0.4126 - val_accuracy: 0.8447 - 22s/epoch - 168ms/step\n",
      "Epoch 21/32\n",
      "133/133 - 23s - loss: 0.2048 - accuracy: 0.9087 - val_loss: 0.4088 - val_accuracy: 0.8513 - 23s/epoch - 170ms/step\n",
      "Epoch 22/32\n",
      "133/133 - 22s - loss: 0.1980 - accuracy: 0.9121 - val_loss: 0.4263 - val_accuracy: 0.8433 - 22s/epoch - 168ms/step\n",
      "Epoch 23/32\n",
      "133/133 - 23s - loss: 0.2000 - accuracy: 0.9095 - val_loss: 0.4294 - val_accuracy: 0.8456 - 23s/epoch - 170ms/step\n",
      "Epoch 24/32\n",
      "133/133 - 23s - loss: 0.1903 - accuracy: 0.9116 - val_loss: 0.4322 - val_accuracy: 0.8442 - 23s/epoch - 172ms/step\n",
      "Epoch 25/32\n",
      "133/133 - 23s - loss: 0.1932 - accuracy: 0.9140 - val_loss: 0.4269 - val_accuracy: 0.8485 - 23s/epoch - 173ms/step\n",
      "Epoch 26/32\n",
      "133/133 - 23s - loss: 0.1816 - accuracy: 0.9168 - val_loss: 0.4571 - val_accuracy: 0.8456 - 23s/epoch - 169ms/step\n",
      "Epoch 27/32\n",
      "133/133 - 22s - loss: 0.1854 - accuracy: 0.9140 - val_loss: 0.4514 - val_accuracy: 0.8357 - 22s/epoch - 166ms/step\n",
      "Epoch 28/32\n",
      "133/133 - 22s - loss: 0.1902 - accuracy: 0.9134 - val_loss: 0.4653 - val_accuracy: 0.8490 - 22s/epoch - 167ms/step\n",
      "Epoch 29/32\n",
      "133/133 - 22s - loss: 0.1741 - accuracy: 0.9175 - val_loss: 0.4834 - val_accuracy: 0.8475 - 22s/epoch - 166ms/step\n",
      "Epoch 30/32\n",
      "133/133 - 22s - loss: 0.1808 - accuracy: 0.9163 - val_loss: 0.4801 - val_accuracy: 0.8438 - 22s/epoch - 168ms/step\n",
      "Epoch 31/32\n",
      "133/133 - 22s - loss: 0.1950 - accuracy: 0.9103 - val_loss: 0.4522 - val_accuracy: 0.8404 - 22s/epoch - 168ms/step\n",
      "Epoch 32/32\n",
      "133/133 - 23s - loss: 0.1754 - accuracy: 0.9160 - val_loss: 0.4983 - val_accuracy: 0.8428 - 23s/epoch - 170ms/step\n",
      "Score for fold 5: loss of 0.4983291029930115; accuracy of 84.28030014038086%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.45839089155197144 - Accuracy: 85.18694043159485%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4538726806640625 - Accuracy: 85.46401262283325%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47158581018447876 - Accuracy: 84.5643937587738%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.45820730924606323 - Accuracy: 86.22159361839294%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4983291029930115 - Accuracy: 84.28030014038086%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 85.14344811439514 (+- 0.685183082880079)\n",
      "> Loss: 0.46807715892791746\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1\n",
    "for train, test in kfold.split(x, y):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(4000, 64, input_length=x.shape[1]))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(LSTM(100, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.2))\n",
    "    #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.3))\n",
    "    model.add(Dense(5, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    # Fit data to model\n",
    "    history = model.fit(x[train], y[train], epochs=32, batch_size=64,validation_data=(x[test], y[test]), verbose=2)\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x[test], y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(acc_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Average scores for all folds:')\n",
    "    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 38ms/step - loss: 0.4983 - accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x[test], y[test], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4983291029930115, 0.8428030014038086]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 4s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=metrics.confusion_matrix((y[test]).argmax(axis=1), predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 93. 120.  29.  62.  28.]\n",
      "[92. 99. 32. 81. 28.]\n",
      "[580. 750. 118. 267.  65.]\n",
      "[1347. 1143. 1933. 1702. 1991.]\n"
     ]
    }
   ],
   "source": [
    "print(FP)\n",
    "print(FN)\n",
    "print(TP)\n",
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1_score = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall   [0.86309524 0.88339223 0.78666667 0.76724138 0.69892473]\n",
      "precision [0.86181278 0.86206897 0.80272109 0.81155015 0.69892473]\n",
      "f1_score [0.86245353 0.87260035 0.79461279 0.788774   0.69892473]\n"
     ]
    }
   ],
   "source": [
    "print(\"recall  \", recall)\n",
    "print(\"precision\",precision)\n",
    "print(\"f1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428030303030303"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold as kf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "def cross_validation(history, _X, _y, _cv=5):\n",
    "      '''Function to perform 5 Folds Cross-Validation\n",
    "       Parameters\n",
    "       ----------\n",
    "      model: Python Class, default=None\n",
    "              This is the machine learning algorithm to be used for training.\n",
    "      _X: array\n",
    "           This is the matrix of features.\n",
    "      _y: array\n",
    "           This is the target variable.\n",
    "      _cv: int, default=5\n",
    "          Determines the number of folds for cross-validation.\n",
    "       Returns\n",
    "       -------\n",
    "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "       'recall', 'f1' for both training set and validation set.\n",
    "      '''\n",
    "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "      results = cross_validate(estimator=history,\n",
    "                               x_tarin=_X,\n",
    "                               y_train=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "      \n",
    "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "              \"Training Precision scores\": results['train_precision'],\n",
    "              \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "              \"Training Recall scores\": results['train_recall'],\n",
    "              \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "              \"Training F1 scores\": results['train_f1'],\n",
    "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "              \"Validation Precision scores\": results['test_precision'],\n",
    "              \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "              \"Validation Recall scores\": results['test_recall'],\n",
    "              \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "              \"Validation F1 scores\": results['test_f1'],\n",
    "              \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Bar Chart for both training and validation data\n",
    "def plot_result(x_label, y_label, plot_title, train_data, val_data):\n",
    "        '''Function to plot a grouped bar chart showing the training and validation\n",
    "          results of the ML model in each fold after applying K-fold cross-validation.\n",
    "         Parameters\n",
    "         ----------\n",
    "         x_label: str, \n",
    "            Name of the algorithm used for training e.g 'Decision Tree'\n",
    "          \n",
    "         y_label: str, \n",
    "            Name of metric being visualized e.g 'Accuracy'\n",
    "         plot_title: str, \n",
    "            This is the title of the plot e.g 'Accuracy Plot'\n",
    "         \n",
    "         train_result: list, array\n",
    "            This is the list containing either training precision, accuracy, or f1 score.\n",
    "        \n",
    "         val_result: list, array\n",
    "            This is the list containing either validation precision, accuracy, or f1 score.\n",
    "         Returns\n",
    "         -------\n",
    "         The function returns a Grouped Barchart showing the training and validation result\n",
    "         in each fold.\n",
    "        '''\n",
    "        \n",
    "        # Set size of plot\n",
    "        plt.figure(figsize=(12,6))\n",
    "        labels = [\"1st Fold\", \"2nd Fold\", \"3rd Fold\", \"4th Fold\", \"5th Fold\"]\n",
    "        X_axis = np.arange(len(labels))\n",
    "        ax = plt.gca()\n",
    "        plt.ylim(0.40000, 1)\n",
    "        plt.bar(X_axis-0.2, train_data, 0.4, color='blue', label='Training')\n",
    "        plt.bar(X_axis+0.2, val_data, 0.4, color='red', label='Validation')\n",
    "        plt.title(plot_title, fontsize=30)\n",
    "        plt.xticks(X_axis, labels)\n",
    "        plt.xlabel(x_label, fontsize=14)\n",
    "        plt.ylabel(y_label, fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Intent_Classification_using_LSTM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "[[4.9353554e-04 1.0295045e-05 9.9947828e-01 1.5720603e-05 2.3070850e-06]] Suggestion\n"
     ]
    }
   ],
   "source": [
    "new_intent = ['Kiyyoo dirama oso  eguti jirun na darbe tari ayaana moo sababa godhatu moyuu']\n",
    "seq = tokenizer.texts_to_sequences(new_intent)\n",
    "padded = pad_sequences(seq, maxlen=6000)\n",
    "pred = model.predict(padded)\n",
    "Intent = ['Positive', 'Negative', 'Suggestion', 'Question', 'Wish']\n",
    "print(pred, Intent[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiyyoo dirama oso  eguti jirun na darbe tari ayaana moo sababa godhatu moyuu [4.6773512e-05 5.5271631e-01 4.6204414e-02 3.2389617e-01 7.7136315e-02] [0 1 0 0 0]\n",
      "Meeti ree kuta 28ffaa [3.3783816e-02 1.7257239e-04 9.6589667e-01 1.1430731e-04 3.2723496e-05] [0 0 1 0 0]\n",
      "Kutaa digdami sadetaffa isiin egee dadhabee. [2.4617006e-04 3.9727831e-05 9.9963427e-01 4.0669172e-05 3.9046161e-05] [0 0 1 0 0]\n",
      "Kutaa digdami sadetaffaa fiidagaa maali nuuraa tuursiftaani bar ani 1faa haanga 27faa haarfoofan tuuree [9.1309536e-01 7.0152775e-04 8.3683766e-02 2.5065427e-03 1.2686354e-05] [1 0 0 0 0]\n",
      "Nuuf jabbadhakaa warii Akka kotti kiyyoo jaalatan  [9.9705076e-01 2.8400256e-03 2.2357681e-05 8.6676002e-05 1.7396891e-07] [0 1 0 0 0]\n",
      "Bayyee namatti tola itti fufa gariidha  [9.6439713e-01 4.8355426e-04 3.4712419e-02 3.9788231e-04 9.0149706e-06] [0 1 0 0 0]\n",
      "Diraamichi torbetti yeroo lamaa nuuf dhiyaachuu qaba [9.9335879e-01 6.4165974e-03 9.0590846e-05 1.3377126e-04 2.6801160e-07] [1 0 0 0 0]\n",
      "Ani dhugaa dubbachuuf baayyeen diraamaa kana jaalladhee ilaalaa jira [9.9456936e-01 2.4305910e-03 1.6427203e-03 1.3471782e-03 1.0210733e-05] [0 1 0 0 0]\n",
      "Maliif nurraa turtan garuu? [1.0424185e-01 7.6133793e-04 2.1548059e-03 8.7417912e-01 1.8662959e-02] [0 0 0 1 0]\n",
      "Waan ajaaibatti [2.6197713e-06 9.9999154e-01 1.4728577e-07 5.6021704e-06 1.4813691e-07] [0 1 0 0 0]\n",
      "Namati toltuu [2.9131997e-04 2.4341809e-04 9.9932325e-01 4.4129622e-05 9.7889417e-05] [0 0 1 0 0]\n",
      "Torbeetti al tokko waan taheef osoo sa'ati isaa dheeratee xiqqooshe nutti tola   [9.1954541e-07 5.4457641e-01 5.7789653e-06 4.5540306e-01 1.3792151e-05] [0 1 0 0 0]\n",
      "Rabbi isinirraa haa jaalatu   [0.00628582 0.02454841 0.00979729 0.9365427  0.02282584] [0 0 1 0 0]\n",
      "Namni akka koo dirama kiyoo jaalatan essa jirtu [0.56734574 0.07874408 0.08991724 0.26004326 0.00394977] [0 0 0 1 0]\n",
      "Nurraa turtan  [0.0061718  0.5507835  0.1607053  0.28074986 0.00158952] [0 0 0 1 0]\n",
      "Baayee isiin jaaladha [6.2060190e-06 8.3070797e-01 2.5097183e-05 1.6918464e-01 7.6072887e-05] [0 1 0 0 0]\n",
      "Baayee namatti tolaa waan ta'ef nuuf jabaadhaa  [7.7641394e-05 9.9824452e-01 2.7040225e-05 1.5975994e-03 5.3212290e-05] [0 1 0 0 0]\n",
      "Baayyee gariidha itti nuf fufa [0.5100565  0.05140527 0.33390403 0.05975071 0.04488353] [0 0 0 1 0]\n",
      "Sinan dhabuun dhukuba guddaa dha [0.00479663 0.08850809 0.00233537 0.8920572  0.01230267] [0 0 0 1 0]\n",
      "Jiradhaa [7.0172728e-06 9.9919456e-01 2.1978251e-07 7.9809607e-04 1.3838132e-07] [0 1 0 0 0]\n",
      "Diraamaa akana namaa galu haaruman argee [0.4729784  0.1341338  0.18312737 0.06604183 0.14371859] [1 0 0 0 0]\n",
      "Baayee natti toleeraa kiyyoon [4.3535679e-06 9.9999428e-01 2.2037929e-08 1.3139825e-06 3.7966590e-08] [0 1 0 0 0]\n",
      "Adaan hin murinaa [2.3703170e-01 7.6013511e-01 1.5227760e-03 1.3018536e-03 8.4830208e-06] [1 0 0 0 0]\n",
      "Isini yaddee maaf turtanii  [3.9931292e-06 9.9998605e-01 3.9896911e-08 9.8356240e-06 4.6118910e-08] [0 1 0 0 0]\n",
      "Baayee sin jaladhaa  [9.1260430e-05 9.9605078e-01 2.1898294e-04 3.6329117e-03 6.0939378e-06] [0 1 0 0 0]\n",
      "Baga Nagaan nuuf dhuftan ebbiffama  [0.4487612  0.31882203 0.03668106 0.1834875  0.01224829] [1 0 0 0 0]\n",
      "Anaa dhufuu jeneraa diramaa kiyyoo  [1.1261768e-03 7.9199500e-02 1.1592940e-02 9.0805316e-01 2.8208671e-05] [0 1 0 0 0]\n",
      "Bayye nama tolaa itti fufaa [0.17356004 0.05061764 0.11092155 0.1194941  0.5454067 ] [0 0 0 1 0]\n",
      "Matii Kiyyoo bagaa nuf debitaan  [4.4747976e-06 9.3455523e-01 2.9241232e-06 6.5434977e-02 2.4870922e-06] [0 1 0 0 0]\n",
      "Jabaadhaa yeroo hunda isin daawwataa jirra [1.8457244e-05 9.9998033e-01 1.1647556e-07 1.0416302e-06 5.1005671e-08] [0 1 0 0 0]\n",
      "Dhugaa fira'oliin garaa na nyaatee  [5.1393218e-02 9.4276363e-01 2.0564844e-04 5.6340368e-03 3.4537479e-06] [0 1 0 0 0]\n",
      "Kunis diramadhamoo maaf gabaabsitanii [1.1047525e-01 8.7922162e-01 5.3325398e-03 4.6664258e-03 3.0415773e-04] [0 1 0 0 0]\n",
      "Taatowan diraamaa kanaa nama ogummaa cimaa qabudha [1.7016889e-06 9.9679750e-01 1.3583535e-07 3.2005399e-03 2.2426410e-07] [0 1 0 0 0]\n",
      "Utumaan eguu argadhee   [2.3662383e-02 9.7520715e-01 2.4984806e-04 7.8113610e-04 9.9556863e-05] [0 1 0 0 0]\n",
      "Kutaan Wal jala fuudhamee jira sirreeffadhaa [5.1501575e-03 5.9307099e-04 9.9298120e-01 3.3644761e-04 9.3921664e-04] [0 0 1 0 0]\n",
      "Maatiin diraamaa kiyyoo hedduu namatti toltu [3.9651932e-06 9.9999416e-01 2.0757637e-08 1.9115121e-06 3.6126952e-08] [0 1 0 0 0]\n",
      "Seenaa Artii Oromoo Kesssaatti kan akka diraamaa Kiyyoo bayee namaatti tolu bifa kaaminu baareeda taee hin argine [9.5086170e-06 9.9998713e-01 6.2891232e-08 3.3258978e-06 3.1036066e-08] [0 1 0 0 0]\n",
      "Kana caalaa Jabaadha  [9.9976593e-01 2.1859437e-04 9.4870966e-06 5.9125550e-06 1.3197209e-08] [1 0 0 0 0]\n",
      " Maaloo itti nuu fufa akka hin xumuramne [1.1225580e-05 9.9994135e-01 4.5180403e-07 4.6721536e-05 2.0286210e-07] [0 1 0 0 0]\n",
      " Nu jalaa baddan maal rakkoo maaliitu isin mudateee [0.08363541 0.31643364 0.5254197  0.05246164 0.02204962] [1 0 0 0 0]\n",
      "Anaa dhufuu maalif nu jalaa turtani [2.5599908e-02 1.9644591e-04 9.7324795e-01 6.7016360e-04 2.8558515e-04] [0 0 1 0 0]\n",
      " Ana dhufuu diraamaa kiyyoo baayeen sin egaa turee  [7.76412487e-01 1.00091875e-01 7.50348391e-03 1.15923606e-01\n",
      " 6.85533232e-05] [0 1 0 0 0]\n",
      " Dafaa itti fufaa [8.9827616e-04 9.7800232e-02 3.8786774e-04 9.0016073e-01 7.5290073e-04] [0 0 0 1 0]\n",
      " Kiyyoo torbee kanaa maliif aftaan? [7.8513312e-06 9.9998283e-01 1.4860140e-07 8.8698516e-06 3.6380052e-07] [0 1 0 0 0]\n",
      "wagaa kuma tokko nuuf jiradhaa [4.91370272e-04 9.60684299e-01 1.02272104e-04 3.86774577e-02\n",
      " 4.45320147e-05] [0 1 0 0 0]\n",
      "Monet Wani Rakko tokko illee kan qabdu natti hin Fakkaatu turee Garuu hardhaa irratti  bare  [0.00244094 0.01013647 0.00135509 0.9543033  0.03176409] [0 0 0 1 0]\n",
      "Torbee tokko irraa torbee lama ma nurraa gootan jalallaa keessan torbee lama miti guyyan lama nuti gudatee [9.9639231e-01 2.1114363e-03 7.7932136e-04 7.1219704e-04 4.7509693e-06] [0 1 0 0 0]\n",
      "Ani Obsaa dhabban jiraam ummata koo [1.0743160e-05 8.2587022e-01 2.9861952e-05 1.7229751e-01 1.7916840e-03] [0 0 0 0 1]\n",
      "Maaliif beeksisaa baayiftuu [0.8995645  0.00925234 0.0043909  0.08546714 0.00132507] [0 0 0 1 0]\n",
      "Jabaadhaa bayyee namati tolaa [4.8754137e-05 9.9990642e-01 5.6773615e-07 4.4055607e-05 1.7098469e-07] [0 1 0 0 0]\n",
      "Baga nagaan fuftan hundi kesanu [9.1912454e-01 7.6491694e-04 3.2867029e-02 4.7234189e-02 9.2887703e-06] [0 1 0 0 0]\n",
      "Haawwidhaan isiin egnaa nagaan kotaa [9.8753148e-01 2.1125027e-03 3.6227223e-03 6.7293397e-03 4.0595260e-06] [0 1 0 0 0]\n",
      "Osoon isin eeguu  ijji koo cuummaa na yaase [9.7379962e-04 1.6769273e-02 4.1292454e-04 9.8110181e-01 7.4221939e-04] [0 1 0 0 0]\n",
      "Baga nagaan dhuftan drama kiyyoo [3.8042371e-03 5.4710999e-05 9.9588311e-01 1.7515602e-04 8.2815211e-05] [1 0 0 0 0]\n",
      "Baayeen jaaladhaa diramaa kana itti fufaa [0.3690597  0.29375443 0.13692886 0.17039056 0.02986638] [0 0 1 0 0]\n",
      "Jabadhaa itii nuu fufaa Bayee isin jalanaa [1.9689425e-04 3.9581788e-05 9.9710006e-01 2.4370267e-03 2.2639922e-04] [0 1 0 0 0]\n",
      "Maaloo nuuf dheresaa osso qufin dhumaa [8.9673558e-06 9.8234677e-01 1.3177250e-05 1.7629942e-02 1.2157705e-06] [0 1 0 0 0]\n",
      "Nagaan kotuu lubuko [1.0166937e-05 9.8396021e-01 1.2444584e-05 1.6008290e-02 8.8787438e-06] [0 1 0 0 0]\n",
      "Torbaniti si'a lamaa nuuf godhaa [6.2634663e-06 9.9998748e-01 5.6804076e-08 6.1826004e-06 4.7776201e-08] [0 1 0 0 0]\n",
      "Baga Nagaan Dhuftan Garuu Torbee Darbe Maaf Baddani [4.2157575e-05 9.9995089e-01 2.0940504e-07 6.7223177e-06 6.6421187e-08] [0 1 0 0 0]\n",
      "nurraa hin turiinaa umata koo [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      "Diraamaa afaan Oromo kessaa kiyyoo fi hiree heddu jaaladhera  [4.5641619e-03 9.9536860e-01 3.8561680e-05 2.6513646e-05 2.2153815e-06] [0 0 0 1 0]\n",
      "Yaa umatakoo mee wali haa jajabeesinuu mati kiyyaa naafi ta'a [9.9940014e-01 5.4253818e-04 2.8175988e-05 2.9160799e-05 2.7495126e-08] [1 0 0 0 0]\n",
      "Baga nagaan dhuftan [1.6366264e-04 3.5858966e-02 1.6324861e-04 9.5847088e-01 5.3433543e-03] [0 0 0 1 0]\n",
      "Bagaa nagadhaan dhuftan maati koo [0.00422743 0.9781959  0.0026994  0.01142039 0.0034568 ] [0 1 0 0 0]\n",
      "Bagaa nagaan dhuftaan  [0.4028726  0.2804394  0.10155293 0.19640958 0.01872552] [1 0 0 0 0]\n",
      " Jaalatamtotaa koo torben lama waggaa lama nati ta'ee  [4.0191671e-04 1.0703138e-02 3.7289810e-04 6.1621827e-01 3.7230381e-01] [0 0 0 1 0]\n",
      "Jabaadha oromoo koo [4.4373191e-06 9.9999261e-01 3.9564728e-08 2.7945603e-06 6.2669812e-08] [0 1 0 0 0]\n",
      "Baga nagaan dhuftanii garuu diraamaa keessan bicuu nuuf dheresaa malee [0.13526312 0.4738052  0.07746971 0.09205793 0.22140403] [1 0 0 0 0]\n",
      " Anaa dhufuu  kiyyoo bayee isin yadee  [9.2049710e-02 2.4087084e-03 8.9444977e-01 1.0511319e-02 5.8047211e-04] [0 0 0 1 0]\n",
      "Hojiin meelaalaa natti tolaa hin jiru  [0.12250974 0.7417228  0.00954889 0.11841393 0.00780459] [0 1 0 0 0]\n",
      "Baga nagaan dhuftanii  ijolle kenyaa [0.28477082 0.31364965 0.10829936 0.10259955 0.1906806 ] [0 0 0 0 1]\n",
      "Baga nagaan dhuftan bayyen isiin yadee [0.3690597  0.29375443 0.13692886 0.17039056 0.02986638] [0 1 0 0 0]\n",
      "Kutaa 27 malif lakofsa irranfatani [9.8558772e-01 1.3806734e-02 4.6674380e-05 5.5868342e-04 2.0984469e-07] [1 0 0 0 0]\n",
      "Baga nagaan dhuftani akka itti isin yadee anatu bekaa [6.0815954e-01 3.5843211e-01 6.9903666e-03 2.5865186e-02 5.5282505e-04] [0 1 0 0 0]\n",
      "Baga nagaa dhufnii [6.3422305e-04 9.9443817e-01 3.6498671e-04 4.5606438e-03 1.9889471e-06] [0 0 1 0 0]\n",
      "Diraamaa midhagaa dha nuuf jabaadhaa [6.15597528e-05 6.00906787e-03 9.58591118e-05 9.81383383e-01\n",
      " 1.24500925e-02] [0 0 0 1 0]\n",
      "Diraama kana  irraa wan bayyee irraa baradhe jabaadha  ittii fufaa [8.2313444e-04 9.8614955e-01 2.9740296e-03 9.7499425e-03 3.0340857e-04] [0 0 1 0 0]\n",
      "Diraamaa kiyooo anaa dhufuuu [7.7825935e-06 9.9996138e-01 1.4341504e-07 3.0679999e-05 9.6428757e-08] [0 1 0 0 0]\n",
      "Jabaadhaa [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      "Torbee nuraa hin dabarsiinaa [0.13199353 0.0079088  0.03585696 0.746825   0.07741582] [1 0 0 0 0]\n",
      "Maalif nuraa turtaan [5.7491404e-04 5.7712127e-04 9.9878615e-01 3.8109516e-05 2.3647219e-05] [0 0 1 0 0]\n",
      "Ana dhufuu jaalatamtoota keenyaa  [1.5900979e-06 9.4696885e-01 1.4142920e-06 5.3025566e-02 2.6260670e-06] [0 0 0 1 0]\n",
      "Kijibanii akka robsan kanaa hin jiruu [0.00479663 0.08850809 0.00233537 0.8920572  0.01230267] [0 0 0 1 0]\n",
      "Jabaadhaa [0.5174247  0.00825417 0.00967432 0.46032995 0.00431686] [0 0 0 1 0]\n",
      "Baga nagaan dhuftan [0.47911602 0.5052989  0.0069517  0.00695208 0.00168133] [0 0 1 0 0]\n",
      "Torbee osso hin dhufin haafe garuu maal taatan [1.2521775e-02 9.8611742e-01 1.1387677e-04 1.2455620e-03 1.2444607e-06] [0 1 0 0 0]\n",
      "Baayyee isiin yaadne [3.4465289e-03 9.4374800e-01 1.5007675e-04 5.2640997e-02 1.4391664e-05] [0 1 0 0 0]\n",
      " isiin hordofuun keenyaa guddina aartii fi artistoota keenyaaf jajabinaa jedheen yaada  [3.9743107e-05 9.9995005e-01 1.2782834e-06 8.7872859e-06 6.3679138e-08] [0 0 0 1 0]\n",
      "Baga naga dhuftaan [0.01049184 0.00717945 0.00897106 0.04691115 0.9264465 ] [0 1 0 0 0]\n",
      "Sinaan mucatii lafaa raa ishee kasee akkam baredi [6.5527238e-05 3.0282259e-01 5.1835126e-05 6.9691288e-01 1.4712589e-04] [0 0 0 1 0]\n",
      "Nuuf Jiraadhaa  [8.7618497e-07 6.2788254e-01 4.7008352e-06 3.7209609e-01 1.5754216e-05] [0 0 0 1 0]\n",
      "Ni turtanii maal rakoon jira moo [8.1610335e-05 9.9972779e-01 7.0963750e-07 1.8958621e-04 1.9950761e-07] [0 1 0 0 0]\n",
      "Baga nagaa dhuftan  [0.4940747  0.18904585 0.10948482 0.20363292 0.0037617 ] [0 1 0 0 0]\n",
      "Maloo maaf nuraa  turtani  bayyee isin yadee bar [1.4613694e-04 1.4801753e-01 7.9448469e-04 8.1005377e-01 4.0988054e-02] [0 0 0 1 0]\n",
      "Waali xaaxxeraa dubiin gaariimaa haa ta.uu malee fuuldurii isaa [2.1179134e-05 9.9997520e-01 2.2149381e-07 3.2359728e-06 8.8549676e-08] [0 0 0 1 0]\n",
      "Robsan baguma mana san hin seenin male si jala dhiphataa ture  [0.00176931 0.14555743 0.00136597 0.8425765  0.00873077] [0 0 1 0 0]\n",
      "Mee namnii akaa kooo dirmaaa kiyyoo arguuf arifatuu essa jirtuu [0.04508301 0.32319468 0.02635996 0.11189652 0.4934658 ] [0 0 0 0 1]\n",
      "Baga nagaan dhuftan   baayee isiin yaadee [0.0530606  0.06036847 0.0543784  0.08333224 0.74886024] [0 1 0 0 0]\n",
      "Maaliif akka fincaan aduree gootu dheressa male [1.7659600e-04 6.7481777e-04 6.9237816e-05 9.9865192e-01 4.2746527e-04] [0 1 0 0 0]\n",
      "Kutaa 27ffaa moo 28tii kuni garuu [4.9938756e-04 9.9932599e-01 2.3042578e-06 1.7196732e-04 3.1947289e-07] [0 1 0 0 0]\n",
      "Diraamaan keessan  akkuma tapha ijollee tahaa jiraa sirressa [0.13851021 0.76996183 0.01039374 0.07419491 0.00693937] [0 1 0 0 0]\n",
      "Hin gabaabbatee xiqqoo yeroo itti dabalaa [0.3859544  0.5276901  0.0044077  0.08082487 0.00112301] [0 1 0 0 0]\n",
      "Ana dhufuu heduu isin yaadnee jiraa [1.56391379e-05 9.59844291e-01 1.09040175e-05 4.00773175e-02\n",
      " 5.17969966e-05] [0 0 0 1 0]\n",
      " baayyee namatti tola garuu beeksiisa hin dheeressina malee [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      "Diramaa akkaa  bishani dhebonuu Diraamaan kun [8.9491183e-01 4.6898030e-02 2.7331492e-02 3.0277913e-02 5.8072549e-04] [0 1 0 0 0]\n",
      "Torbee taa'en issin eegaan bule [4.0539348e-01 5.7311356e-01 1.0310339e-02 1.0903315e-02 2.7928085e-04] [0 0 1 0 0]\n",
      "Baayee namatti toltuu [9.1702423e-06 9.9997568e-01 1.0272377e-07 1.4918111e-05 6.4671760e-08] [0 1 0 0 0]\n",
      "Hawwi gudaan isin egga turee bagaa nagaan dhuftan [6.5527238e-05 3.0282259e-01 5.1835126e-05 6.9691288e-01 1.4712589e-04] [0 0 0 1 0]\n",
      "Baayee issin jaaladhaa  oromoo koo [0.00276163 0.10651751 0.00095067 0.8815874  0.00818279] [0 1 0 0 0]\n",
      "Diraamaan kunii heduu  namatii  tolaa  jabbesa  itti  fufa  [0.00479663 0.08850809 0.00233537 0.8920572  0.01230267] [0 0 0 1 0]\n",
      "Baayee namatti toltuu dhugaa [2.1412834e-03 9.9752444e-01 1.9346413e-05 3.0914080e-04 5.7473512e-06] [0 1 0 0 0]\n",
      "Baga Nagaan dhuftan torbee maa Nuraa haftaan [6.6689172e-06 5.3543046e-02 1.1278300e-05 9.4640142e-01 3.7528102e-05] [0 0 0 1 0]\n",
      "Baga nagan dhuftanii garuu nuraa turtanii  [8.6197156e-01 1.2980839e-03 3.3341767e-03 1.3303496e-01 3.6119719e-04] [0 0 1 0 0]\n",
      "Xiqqoo otoo akkanatti addaan turuu baattanii namatti tola [4.0172245e-06 6.6637599e-01 4.1682351e-06 3.3359733e-01 1.8473171e-05] [0 0 0 1 0]\n",
      "Baayee namati tola dhugaa [3.2411334e-01 6.7446095e-01 1.0852513e-03 3.3968160e-04 7.8697326e-07] [0 1 0 0 0]\n",
      "Galatomaa  waan bohasitanfi itti fufa [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      "Dhugaa hedduu isin eegaa turree  [1.35229020e-05 9.99933720e-01 1.12699766e-07 5.26576368e-05\n",
      " 2.40380871e-08] [0 1 0 0 0]\n",
      "Baga nagaan dhuftaan kabajamoo keenyaa  siin yaaderaa  [1.2418265e-04 9.8491353e-01 3.7744448e-03 1.1156541e-02 3.1362022e-05] [0 1 0 0 0]\n",
      "Baayee Namati toltuu baaye sin jaladha [2.6789915e-06 9.9999547e-01 1.6616065e-08 1.7302410e-06 4.7254655e-08] [0 1 0 0 0]\n",
      "Yeroon beksisumaan dhume [6.3679482e-07 7.8222460e-01 1.5593256e-06 2.1775416e-01 1.9044037e-05] [0 0 0 1 0]\n",
      "Bayyee isinii jaladhaa diraamaa kiyyoo kesatuu meluu [2.9025737e-06 9.9996901e-01 5.0056453e-08 2.8069961e-05 4.8424486e-08] [0 1 0 0 0]\n",
      "Anaa dhufu baayeen isiin yaadee [2.25377022e-04 4.13191228e-05 9.99620199e-01 1.08026115e-05\n",
      " 1.02386031e-04] [0 0 1 0 0]\n",
      "Baga nagaan dhuftan [0.35234165 0.46279114 0.08117525 0.09168759 0.01200438] [0 1 0 0 0]\n",
      "Baayee namatii tolaa jajabadhaa ittii fufaa [3.2327889e-04 4.0071711e-01 9.5134862e-03 1.9666165e-01 3.9278445e-01] [0 0 0 0 1]\n",
      "Essatti badan kana hundaa obsa na ficisistan   [0.17181452 0.00364325 0.00198991 0.8186344  0.00391788] [1 0 0 0 0]\n",
      "Maatii kiyyoo dhugaa fuundura saa tilmaamuun ni ulfaata [0.00895136 0.81214786 0.15727517 0.01054221 0.01108339] [0 0 1 0 0]\n",
      "Baga nagaan dhuftan maatii kiyyoo  [2.6501506e-05 9.9588674e-01 4.2586598e-06 4.0809144e-03 1.5142463e-06] [0 1 0 0 0]\n",
      "Kana caalaa jabaadhaa oromoo koo [0.10744854 0.31509283 0.06720701 0.07044766 0.43980396] [0 0 0 0 1]\n",
      "Diramaan kuni akkumaa maqaasaa kiyyoodha  [1.7085646e-03 9.6300584e-01 2.6820277e-04 3.4902431e-02 1.1505375e-04] [0 0 0 0 1]\n",
      " Ana haa dhufuu baga nagaan dhuftan [3.3238501e-04 3.5133368e-01 2.2548968e-04 6.4777738e-01 3.3112452e-04] [0 1 0 0 0]\n",
      "Yeroo hin gabaabsinaa  [9.2727727e-01 6.9703825e-02 7.7369297e-04 2.2145300e-03 3.0772662e-05] [1 0 0 0 0]\n",
      "Melaalaa irraa Anuu qabdaa [5.9109067e-07 5.5613852e-01 2.9919117e-06 4.4383565e-01 2.2296310e-05] [0 1 0 0 0]\n",
      "Namicha Diaspora  Kana osoo kessa baafatanii nati tola  [0.7178829  0.20548475 0.03172354 0.04315434 0.00175453] [1 0 0 0 0]\n",
      "Barreeffama 28 gara 27 jijjiiraa [0.31131092 0.46455848 0.12115128 0.10044789 0.0025314 ] [1 0 0 0 0]\n",
      "Heddu isini egaa turre [0.00278461 0.02936343 0.00290581 0.9636757  0.00127047] [0 0 0 1 0]\n",
      "Waantii sinaanitii demaa jiruu ammaale garaa nama nyaata [5.4041024e-02 9.2846572e-01 1.6955145e-02 5.1562709e-04 2.2515072e-05] [0 1 0 0 0]\n",
      "Jabadhaa [3.9042446e-01 5.9772694e-01 1.3009997e-03 1.0303899e-02 2.4368528e-04] [0 0 0 0 1]\n",
      "Torbban tokkoo waggaa natti ta'e [6.5679336e-03 3.4176979e-02 9.0784895e-01 5.1207565e-02 1.9859863e-04] [1 0 0 0 0]\n",
      "29ffa itti nu arifadhaa [1.1132692e-05 9.9980432e-01 4.7572760e-07 1.8379519e-04 3.8029725e-07] [0 0 0 0 1]\n",
      "Akka hadha gaba dhaqxee isiin egnee dadhabne  [2.1259294e-01 4.8342475e-04 7.7763116e-01 9.1860862e-03 1.0644375e-04] [0 0 1 0 0]\n",
      "Baayee nama dhibduu [4.3330986e-02 8.2657943e-03 9.3924654e-01 8.9436285e-03 2.1312856e-04] [0 0 1 0 0]\n",
      "Taatonni diraamaa kanaa hedduu galatoomaa   [8.9803134e-04 4.5487517e-01 2.0667940e-04 4.0208340e-01 1.4193663e-01] [0 1 0 0 0]\n",
      " Torbee darbee maaf nuu jalaa haftaan [6.1640557e-04 1.5270916e-03 3.8292061e-03 4.9648406e-03 9.8906249e-01] [0 0 0 0 1]\n",
      "Dhugaa dubbachuuf diraaman kiyyo kun diraamaa addaati [1.9041900e-01 8.0778295e-01 5.8810820e-04 1.1517147e-03 5.8211277e-05] [0 0 0 1 0]\n",
      " bayyee midhagaa waan isaa hundii [0.25550908 0.43753344 0.12687953 0.14428046 0.03579757] [1 0 0 0 0]\n",
      "galatooma jechaa isinis jajjabeesuun nura jirra [0.8044217  0.04759062 0.09957217 0.04556521 0.0028503 ] [0 0 0 1 0]\n",
      "Yaroo diraamaa itti nuuf dheerressa beeksiisa irraa dhimma hin qabnu [4.04391263e-04 9.99382019e-01 1.12566831e-05 1.91826344e-04\n",
      " 1.04935825e-05] [0 1 0 0 0]\n",
      "Baga nagaan  dhuftan torbe issiin egaan dhabe   [8.69987439e-03 9.89969432e-01 1.06994674e-04 1.19147880e-03\n",
      " 3.22534506e-05] [0 1 0 0 0]\n",
      "jabadhaa hojii kesani hedduu nama bonsaa [8.3699787e-01 1.3939804e-01 1.2864392e-03 2.2170274e-02 1.4732615e-04] [0 0 0 1 0]\n",
      " torbee lama Booda nuf deebitan isin yaadeem  [0.42238533 0.00062368 0.00348045 0.4640323  0.10947815] [0 0 0 0 1]\n",
      "Bonan bilisee sinan irraa fudhatuf dema moo [0.01026836 0.97273177 0.0033668  0.01199056 0.00164256] [0 1 0 0 0]\n",
      "Baayee namattii toltuu nuuf jiradhaa    [1.2143082e-03 1.8784100e-02 7.5528485e-01 2.2466505e-01 5.1672632e-05] [0 0 0 1 0]\n",
      "Isin yaddee dhugaa baga nagaan dhuftan [0.00366426 0.04993495 0.00123309 0.9206126  0.02455503] [0 0 0 0 1]\n",
      "Maloo qalbi nu raraftani torbe addan kutani  [1.6530300e-03 8.4198838e-01 6.3976005e-04 1.5557316e-01 1.4567190e-04] [0 0 0 1 0]\n",
      "Baga Nagan na dhuftaan bayyeen sin jalladhaa [1.52053183e-03 9.98128712e-01 1.47069895e-05 3.32480820e-04\n",
      " 3.52903089e-06] [0 1 0 0 0]\n",
      "Akkuman ittii isiin yadee yoo bektan [0.06130575 0.11714168 0.04954343 0.2944043  0.47760475] [0 0 0 1 0]\n",
      "Baga nagaan dhuftan Garuu  nurraa hin turinaa [0.11231445 0.10991167 0.01049568 0.7567151  0.010563  ] [0 0 0 1 0]\n",
      "Garuu of nu barsiistanii maalif akkana baddan [3.8754171e-01 1.9872964e-03 6.0708243e-01 3.0931688e-03 2.9537961e-04] [0 0 1 0 0]\n",
      "Diraamaa kana komeentii barreessuu qofa osoo hin taane hanga xumurraati lamuu dhan haa jajjabeessinu. [0.0017725  0.00818612 0.00143931 0.01630123 0.9723008 ] [0 0 0 0 1]\n",
      "Dhugaa beyee tokkoo namatii totuu itti fufaa  [3.5440992e-04 8.5566890e-01 1.9688455e-03 1.3837413e-01 3.6337983e-03] [1 0 0 0 0]\n",
      "Anaa dhufu kiyyoo koo sin yaade dhugaa [0.01341005 0.01616038 0.02766995 0.15825945 0.7845002 ] [0 0 0 0 1]\n",
      "Wal baqachuun kun hanga yoomitti garuu [4.210965e-04 9.851522e-03 1.658083e-04 9.465312e-01 4.303042e-02] [0 0 0 1 0]\n",
      "Baga bara haraa gessan ijoollee oromoo  [5.0821873e-05 1.2453040e-04 9.9944550e-01 3.5428008e-04 2.4820150e-05] [0 0 1 0 0]\n",
      "Beeksisa nuttii hin bayyisinaa [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      " kiyyoo akkan sin jaaladhuu  [0.11018069 0.10354231 0.11793143 0.13801211 0.5303335 ] [0 0 0 0 1]\n",
      "Nuraa turtan garuu jabaadhaa namaatii tolaa [0.08570848 0.02149472 0.7555849  0.02484165 0.11237023] [0 0 1 0 0]\n",
      " Natuu duraa lallee jiraa    [2.1927994e-05 9.9993181e-01 3.4456573e-07 4.5558958e-05 3.7580168e-07] [0 1 0 0 0]\n",
      "Kiyyoon kan duraa irraa amma caalatti bareeda jira jabaadha [2.4390311e-04 3.3203691e-01 4.8244045e-05 6.6746080e-01 2.1012772e-04] [0 0 0 1 0]\n",
      "Anatuu jalqabaa lallee  [0.01149604 0.00471708 0.00442144 0.97819304 0.00117237] [0 1 0 0 0]\n",
      "Torbee darbee essatii haaftan [0.5873742  0.03907533 0.10676821 0.23585677 0.03092552] [1 0 0 0 0]\n",
      "malif hin dhufin sanbata taree [1.0317607e-02 4.0230956e-05 9.8937815e-01 1.7255997e-04 9.1383736e-05] [0 0 1 0 0]\n",
      "Mee namnii akka koo hawwiidhan eegata turtan essa jirtuu [0.0878413  0.02305717 0.03317764 0.85346866 0.0024552 ] [0 0 1 0 0]\n",
      "Haaradha Namni tokko nan Dursinee [4.2994617e-04 4.6412933e-05 9.9886727e-01 4.5030136e-04 2.0614410e-04] [0 0 1 0 0]\n",
      " Anaa dhufuu  Diramaa kiyyoo  [3.4245069e-05 5.8623045e-03 1.5001079e-05 9.9366969e-01 4.1879519e-04] [0 0 0 1 0]\n",
      "An dhufuu kabajamoota keenyaa matii kiyyoo [9.1745156e-01 3.1930916e-03 7.4830495e-02 4.4914354e-03 3.3369204e-05] [0 0 1 0 0]\n",
      "Anaa dhufuu qopheesitoota diraama Kiyyoo [2.3586866e-03 1.5504511e-02 6.9700241e-02 9.1242951e-01 6.9822518e-06] [1 0 0 0 0]\n",
      "Maaf torbee Darbee nuraa haftanii osso isinii egnuu [5.0609225e-07 1.3382593e-01 1.8992266e-06 8.6613864e-01 3.3081225e-05] [0 0 0 1 0]\n",
      "Sin eegaa jirra dheebun [1.3101868e-03 9.9693871e-01 1.4581647e-05 1.7348218e-03 1.7315134e-06] [0 1 0 0 0]\n",
      "Heddu na arsitanii  [8.7633187e-01 5.6360671e-03 1.0956494e-01 8.2750302e-03 1.9211401e-04] [0 0 1 0 0]\n",
      "Anatu tokofaa lalee [9.4800827e-04 9.9895334e-01 2.8355748e-06 9.4873678e-05 8.3056176e-07] [0 1 0 0 0]\n",
      "Baga nagaan dhuftan garuu hin tursiisinaa [9.9382073e-01 5.3396996e-04 5.4941755e-03 1.4420206e-04 7.0322953e-06] [0 0 1 0 0]\n",
      "Maatiin diraamaa kiyyoo suusii natti taatan hojii keessan nyaadhaa [9.9802232e-01 1.1095927e-03 5.9444580e-04 2.6764863e-04 6.0882767e-06] [1 0 0 0 0]\n",
      "Diramaan kiyyoo bayee  namatii tolaa [6.2992400e-04 7.1608258e-05 9.9906760e-01 4.7397200e-05 1.8358708e-04] [0 0 1 0 0]\n",
      "Issa darbe Kutaa 28 keessatti nan dhabe [9.9793410e-01 1.7620379e-03 2.6141296e-04 4.2239633e-05 2.2231205e-07] [1 0 0 0 0]\n",
      "Baayee namatti tolaa [0.72081304 0.00597112 0.02218083 0.24658576 0.00444927] [0 0 0 1 0]\n",
      "Nujalaa gabaabsitan san gubbaan torbetti al tokko [5.6618415e-02 9.4217724e-01 9.5209667e-05 1.0949168e-03 1.4183755e-05] [1 0 0 0 0]\n",
      "Diramaa kiyoo bayyee isin jaaladhaa [9.8424006e-01 1.0027444e-02 1.7414852e-03 3.9685322e-03 2.2444587e-05] [1 0 0 0 0]\n",
      "Baga dhuftan  [8.6262185e-07 5.5415159e-01 4.6749469e-06 4.4579253e-01 5.0301271e-05] [0 0 0 1 0]\n",
      "Akka isiin jaaladhuu [4.1682065e-02 1.1115990e-03 9.5127410e-01 5.4129590e-03 5.1933515e-04] [1 0 0 0 0]\n",
      "Sagaleen babbadaa jira akka duraati deebisaa [0.4667895  0.31199446 0.07834833 0.13879305 0.00407473] [1 0 0 0 0]\n",
      "Baayee Namatti Tola Galatoma  [9.8211801e-01 2.3982853e-03 8.5719628e-03 6.8582175e-03 5.3515607e-05] [1 0 0 0 0]\n",
      "Siifoo xiqooman argee [9.8322833e-01 2.1685530e-03 1.5414403e-03 1.3042725e-02 1.9017534e-05] [0 0 0 1 0]\n",
      "Bekisisaa xiqqessaa maali kuni hundii [9.9809939e-01 1.8535968e-03 1.6376056e-05 3.0461741e-05 6.4546725e-08] [0 0 1 0 0]\n",
      "Maaf garuu diramaa yeroo hojjatan ni gababataa mafi dheresitan hin hojjatinaa  [9.7325587e-01 1.6124295e-04 6.3625810e-04 2.5873344e-02 7.3379357e-05] [1 0 0 0 0]\n",
      "Isin Jalanaa jajabaadha [0.00527471 0.96712786 0.0150488  0.00802719 0.0045214 ] [0 0 0 0 1]\n",
      "Dafaa nuf dabalaa diramaan kun bayyee namaa hawataa dhugaa  [0.1781929  0.24575873 0.08608139 0.17793545 0.31203163] [1 0 0 0 0]\n",
      "Hedduu namatti tola [9.9498308e-01 2.1473582e-04 4.4702431e-03 3.3016608e-04 1.8211377e-06] [1 0 0 0 0]\n",
      "Maaloo ni gabaabsitani yokin immoo torbeetti yeroo 2 nuuf godhaa  [5.7411727e-02 8.7876589e-04 9.3925452e-01 2.2431086e-03 2.1183994e-04] [0 0 1 0 0]\n",
      "Maal taatanii [0.00331851 0.6299881  0.01034581 0.34242582 0.01392181] [0 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print (data['Text'][i], predictions[i], y[test[i]]) for i in range(0, 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=metrics.confusion_matrix((y[test]).argmax(axis=1), predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FP)\n",
    "print(FN)\n",
    "print(TP)\n",
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1_score = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"recall  \", recall)\n",
    "print(\"precision\",precision)\n",
    "print(\"f1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()\n",
    "\n",
    "def precision_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n",
    "\n",
    "def recall_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"label precision recall\")\n",
    "for label in range(10):\n",
    "    print(f\"{label:4d} {precision(label, cm):9.3f} {recall(label, cm):6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision total:\", precision_macro_average(cm))\n",
    "\n",
    "print(\"recall total:\", recall_macro_average(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_intent = ['Kiyyoo dirama oso  eguti jirun na darbe tari ayaana moo sababa godhatu moyuu']\n",
    "seq = tokenizer.texts_to_sequences(new_intent)\n",
    "padded = pad_sequences(seq, maxlen=6000)\n",
    "pred = model.predict(padded)\n",
    "Intent = ['Positive', 'Negative', 'Suggestion', 'Question', 'Wish']\n",
    "print(pred, Intent[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
